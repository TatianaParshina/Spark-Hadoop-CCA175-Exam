pyspark --master yarn --conf spark.ui.port=12888

orders = sc.textFile("/public/retail_db/orders")
order_items = sc.textFile("/public/retail_db/order_items")
products = sc.textFile("/public/retail_db/products")

ordersMap = orders.map(lambda o: (o.split(",")[0], o.split(",")[1].split(" ")[0], o.split(",")[3]))
order_itemsMap = order_items.map(lambda o: (o.split(",")[1], o.split(",")[2], o.split(",")[4]))
productsMap = products.map(lambda o: (o.split(",")[0], o.split(",")[1], o.split(",")[2]))

ordersMapDF = ordersMap.toDF(schema=["order_id", "order_date", "order_status"])
order_itemsMapDF = order_itemsMap.toDF(schema=["order_item_order_id", "order_item_product_id", "order_item_subtotal"])
productsMapDF = productsMap.toDF(schema=["product_id", "product_category_id", "product_name"])

ordersMapDF.registerTempTable("orders")
order_itemsMapDF.registerTempTable("order_items")
productsMapDF.registerTempTable("products")

res = sqlContext.sql("select distinct order_date, sum(order_item_subtotal) over (partition by product_id) as order_revenue, product_name, product_category_id from orders o join order_items oi on o.order_id = oi.order_item_order_id join products p on oi.order_item_product_id=p.product_id where order_date = '2013-07-26' and order_status in ('COMPLETE', 'CLOSED') order by order_revenue desc limit 5")
res.show()

resMap = res.map(lambda r: str(r[0]) + ":" + str(r[1])+ ":" + str(r[2])+ ":" + str(r[3]))

resMap.saveAsTextFile("/user/tanyaparsh/problem7/solution/")

#validate
sqlContext.read.load("/user/tanyaparsh/problem7/solution/", "text").show()